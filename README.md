# Master's Machine Learning, Harbour.Space University
## Spring 2021

# Overview
This course aims to introduce students to the contemporary state of Machine Learning and Artificial Intelligence. It combines theoretical foundations of Machine Learning algorithms with comprehensive practical assignments. The course covers materials from classical algorithms to Deep Learning approaches and recent achievements in the field of Artificial Intelligence. This course is accompanied by Deep Learning in Applications course (Module 12), which brings the most recent achievements in the field and their applications.

Programming assignments will be implemented in Python 3. PyTorch framework will be used for Deep Learning practice.

Special acknowledgements to Nikolay Karpachev and Ivan Provilkov for contributions into the course materials and structure.

# Learning highlights

Learn the main theoretical foundations of Machine Learning and Deep Learning
Get familiar with various approaches to supervised and unsupervised problems
Gain essential experience in data preprocessing, model development, fitting and validation
Develop skills required in product development and applied research
Learning highlights
Learn the main theoretical foundations of Machine Learning and Deep Learning
Get familiar with various approaches to supervised and unsupervised problems
Gain essential experience in data preprocessing, model development, fitting and validation
Develop skills required in product development and applied research
# Additional resources

### Great books:
1. Deep Learning book (classics, really): https://www.deeplearningbook.org
2. The Hundred-page Machine Learning book: [link](http://themlbook.com) (available online, e.g. on the [github](https://github.com/ZakiaSalod/The-Hundred-Page-Machine-Learning-Book)


Additional materials for self-study:
1. Naive Bayesian classifier explained: [link](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)
2. Stanford notes on linear models: [link](http://cs229.stanford.edu/notes/cs229-notes1.pdf)
3. Detailed description of bootstrap procedure: [link](http://www.math.ntu.edu.tw/~hchen/teaching/LargeSample/notes/notebootstrap.pdf)
4. Bias-variance tradeoff in more general case: A Unified Bias-Variance Decomposition and its Applications [link](https://homes.cs.washington.edu/~pedrod/papers/mlc00a.pdf)
5. Notes on matrix derivatives from Stanford: [link]( http://cs231n.stanford.edu/handouts/derivatives.pdf)
6.  Great interactive blogpost by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html
7.  And great gradient boosted trees playground by Alex Rogozhnikov: http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html
8. Shap values repo and explanation: https://github.com/slundberg/shap
9. Kaggle tutorial on feature importances: https://www.kaggle.com/learn/machine-learning-explainability

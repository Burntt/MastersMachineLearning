{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "###### Name:\n",
    "\n",
    "# Berend Gort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Midterm test and practice session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Questions.\n",
    "Please, answer the following questions briefly. Two or three sentences with main idea would be enough.\n",
    "\n",
    "Do not use external resourses in this part, please. Answer with you own words. If you forgot something, don't worry, we will discuss it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0. \n",
    "Please, formulate the supervised learning problem statement.\n",
    "\n",
    "---\n",
    "\n",
    "##### Answer\n",
    "\n",
    "Based on example input and output pairs, the supervised learning problem statement is the machine learning task that can provide a function that can process new inputs and map them to an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.\n",
    "\n",
    "What are regression and classification problems. Whatâ€™s the difference?\n",
    "\n",
    "---\n",
    "\n",
    "##### Answer\n",
    "\n",
    "- Regression:\n",
    "\n",
    "Regression analysis is the word for describing the set of machine learning tools that provides the user to predict a continous target variable based on the value of one or multiple predictor variables.\n",
    "\n",
    "Mainly, build a mathematical equation where y is the target value, as a function of x variables based on predictors. E.g., linear regression:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x + e(\\beta) $$\n",
    "\n",
    "Where y is the target variable, vector $\\overrightarrow{w}$ the weights vector and x the predictor. Also, e is the noise error on the model.\n",
    "\n",
    "\n",
    "\n",
    "- Classification:\n",
    "\n",
    "Classification is the process of predicting the class of given predictors. It is the task of approximating a mappping function from the predictors to the target. However, in this case the target is discrete. It is also a part of supervised learning.\n",
    "\n",
    "\n",
    "\n",
    "- Difference Regression / Classification:\n",
    "\n",
    "Therefore, the main difference between regression versus classification is that regression predicts continuous quantities, whilst classification predicts discrete class labels and tries to construct hard boundaries between these discrete entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.\n",
    "\n",
    "Write down the linear model for regression problem in matrix notation. What is Mean Squared Error (MSE) loss function? How can it be expressed?\n",
    "\n",
    "---\n",
    "\n",
    "##### Answer\n",
    "\n",
    "The data used consists of n paired observators of the predictor variable X. The response variable is Y. If so, the goal is to fit the model (matrix notation):\n",
    "\n",
    "$$Y = X \\beta + e(\\beta)$$\n",
    "\n",
    "Therefore, as we want to minimize the error ($e(\\beta)$), we can rewrite into:\n",
    "\n",
    "$$e(\\beta) = Y - X \\beta$$\n",
    "\n",
    "The Mean Squared Error estimator, takes the squared error of each datapoint and takes the mean.\n",
    "\n",
    "$$ MSE(\\beta) = \\frac{1}{n} \\sum_{i=1}^n e_i^2(\\beta)$$\n",
    "\n",
    "The corresponding step:\n",
    "\n",
    "$$ S_{MSE} = (X^TX)^{-1}X^TY $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.\n",
    "\n",
    "What is the gradient of a function? How is it being used in optimization?\n",
    "\n",
    "---\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "The gradient of a function is a vector in n-dimensional space that exists of the partial derivatives w.r.t. each dimension and points in the direction of greatest increase of a function.\n",
    "\n",
    "In optimization methods, the gradient is used to minimize or maximimize the value of a function. For example in question 1.2 it is very usefull to minimize the least squared error estimator in order to find the correct weight matrix $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.\n",
    "\n",
    "Write down gradient descent step for linear model and MSE for one-dimensional case.\n",
    "\n",
    "---\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "The linear one-dimensional model is:\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 x + e(\\beta) $$\n",
    "\n",
    "Rewritten to the error value:\n",
    "\n",
    "$$ e(\\beta) = y - (\\beta_0 + \\beta_1 x) $$\n",
    "\n",
    "The MSE estimator which should be minimized:\n",
    "\n",
    "$$ MSE(\\beta) = min \\Big[ \\frac{1}{n} \\sum_{i=0}^n e_i^2(\\beta) \\Big]$$\n",
    "\n",
    "Supplying e into this equation\n",
    "\n",
    "$$ MSE(\\beta) = \\frac{1}{n} \\sum_{i=0}^n (y_i - [\\beta_0 + \\beta_1 x_i])^2 $$\n",
    "\n",
    "Minimize the loss function:\n",
    "\n",
    "$$ L(\\beta) = \\frac{1}{n} \\sum_{i=0}^n (y_i - [\\beta_0 + \\beta_1 x_i])^2 $$\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$\\frac{\\text{d}L}{\\text{d}\\beta} = - \\frac{2}{n}\\sum_{i=0}^n x_i(y_i - \n",
    "\\hat{y_i})$$\n",
    "\n",
    "Step:\n",
    "\n",
    "$$ s_{n+1} =  s_{n} - k \\frac{\\text{d}L}{\\text{d}\\beta} $$\n",
    "\n",
    "Where s is the current function value and k is the step size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.\n",
    "What is validation? Cross validation?\n",
    "\n",
    "---\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "Validation is the process of checking your model on datapoints that were not used when fitting the model.\n",
    "\n",
    "Cross-validation is a technique for assesing how the statistical analysis gerneralises to an independent data set.\n",
    "\n",
    "So its training several models on subsets of the available input data and evaluating them on the test set and look at the performance of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.\n",
    "What is regularization? How does L1 regularization differ from L2 for linear models?\n",
    "\n",
    "---\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "Regularization is the set of techniques that are used to reduce the error by fitting a function on the given training set and avoid overfitting. So when the model learning the noise in the data, we can penalize our loss function by adding a L1 or an L2 norm to our weights vector.\n",
    "\n",
    "\n",
    "The difference between these two regularizations is the penalty term used. L1 adds the penalty equivalent to the sum of the absolute value of the weight coefficients.\n",
    "\n",
    "L2 adds the penalty which is equivalent to the square of the magnitude of the weight coefficients.\n",
    "\n",
    "In linear models, L1 regularization pushes weights of unimportant features to zero, thuss can be seen as reducing the number of features. A form of model complexity.\n",
    "\n",
    "L2 norm can be used when the independent variables are highly correlated. By adding a bias to the estimates, it reduces quality errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7.\n",
    "What are precision and recall metrics?\n",
    "\n",
    "---\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "- Precision\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{True positives}}{\\text{True positives} + \\text{False positives}}$$\n",
    "\n",
    "The fraction of relevant instances among the retrieved instances\n",
    "- Recall\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{True positives}}{\\text{True positives} + \\text{False negatives}}$$\n",
    "\n",
    "The fraction of relevant instances that were retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8.\n",
    "What is bagging? What is the main idea beneath it?\n",
    "\n",
    "---\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "Bagging generates n new trainings sets $X_i$ of the original dataset. After that, train a model on each of the subsets.\n",
    "\n",
    "After that the predictions of each model are combined in to a final, more accurate prediction, because the variance is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.9.\n",
    "How Random Forest is different from simple bagging? \n",
    "\n",
    "---\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "The significant difference is that in Random Forests, merely a subset of features is selected at random out of the total and the best split feature from the subset is used to split each node in the tree.\n",
    "\n",
    "When simple bagging, all features are considered for splitting a node.\n",
    "\n",
    "Therefore, random forests introduce additional variation by taking a sample of the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.10\n",
    "What is gradient boosting? Is it a good idea to use linear regression as a basic model (element of the ensemble) in gradient boosting? What about logistic regression? Why?\n",
    "\n",
    "---\n",
    "\n",
    "##### Answer:\n",
    "\n",
    "Gradient boosting is a method were multiple models are put into an ensemble, just like bagging, BUT sequentally.\n",
    "\n",
    "In each step, the new target is the error value of the existing model. Combining all the models results in the lowest possible prediction error (limited by noise).\n",
    "\n",
    "- Linear regression + Gradient Boosting\n",
    "\n",
    "Gradient boosting ouputs a sequential model of the intermediate step models. Because linear regression is a 'linear' model, the sum of subsequent linear models can also be represented as a single linear regression model. Also, gradient boosting uses the same MSE loss function as linear regression. Therefore, this combination can not ever improve the initial model.\n",
    "\n",
    "- Logistic regression + Gradient Boosting\n",
    "\n",
    "Boosting of logistic regression is ok because the sigmoid function is not a linear function. Therefore the sum of the subsequent model is not a linear model."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

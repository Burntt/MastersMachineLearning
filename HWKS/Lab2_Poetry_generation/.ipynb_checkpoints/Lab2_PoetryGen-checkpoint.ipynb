{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEnQnswVWuEj"
   },
   "source": [
    "## Lab 01. Poetry generation\n",
    "\n",
    "Let's try to generate some poetry using RNNs. \n",
    "\n",
    "You have several choices here: \n",
    "\n",
    "* The Shakespeare sonnets, file `sonnets.txt` available in the notebook directory.\n",
    "\n",
    "* Роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина. В предобработанном виде доступен по [ссылке](https://github.com/attatrol/data_sources/blob/master/onegin.txt).\n",
    "\n",
    "* Some other text source, if it will be approved by the course staff.\n",
    "\n",
    "Text generation can be designed in several steps:\n",
    "    \n",
    "1. Data loading.\n",
    "2. Dictionary generation.\n",
    "3. Data preprocessing.\n",
    "4. Model (neural network) training.\n",
    "5. Text generation (model evaluation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SUsyxbzGWuEr"
   },
   "outputs": [],
   "source": [
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SUsyxbzGWuEr"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWzP9MMsWuEs"
   },
   "source": [
    "### Data loading: Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0Ay8_58WuEs"
   },
   "source": [
    "Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`). Simple preprocessing is already done for you in the next cell: all technical info is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Fy1vzaMqWuEs"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('sonnets.txt'):\n",
    "    !wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/master/homeworks_basic/Lab2_DL/sonnets.txt\n",
    "\n",
    "with open('sonnets.txt', 'r') as iofile:\n",
    "    text = iofile.readlines()\n",
    "    \n",
    "TEXT_START = 45\n",
    "TEXT_END = -368\n",
    "text = text[TEXT_START : TEXT_END]\n",
    "assert len(text) == 2616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTLsAmNfWuEt"
   },
   "source": [
    "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
    "\n",
    "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3AQrivbdWuEt",
    "outputId": "e39f26b2-b053-4c85-f7b0-09834ba27c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "# Join all the strings into one and lowercase it\n",
    "# Put result into variable text.\n",
    "\n",
    "# Your great code here\n",
    "\n",
    "single_text = \"\".join([x.lower() for x in text])\n",
    "\n",
    "assert len(single_text) == 100225, 'Are you sure you have concatenated all the strings?'\n",
    "assert not any([x in set(single_text) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
    "print('OK!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmIh2sHRWuEu"
   },
   "source": [
    "### Data loading: \"Евгений Онегин\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Yfve9e1sWuEu"
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/attatrol/data_sources/master/onegin.txt\n",
    "    \n",
    "# with open('onegin.txt', 'r') as iofile:\n",
    "#     text = iofile.readlines()\n",
    "    \n",
    "# text = [x.replace('\\t\\t', '') for x in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kPIiOD_WuEv"
   },
   "source": [
    "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
    "\n",
    "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "c4pNFKfhWuEw"
   },
   "outputs": [],
   "source": [
    "# Join all the strings into one and lowercase it\n",
    "# Put result into variable text.\n",
    "\n",
    "# Your great code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJe0uDpuWuEw"
   },
   "source": [
    "Put all the characters, that you've seen in the text, into variable `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0a4BZTLEWuEw"
   },
   "outputs": [],
   "source": [
    "tokens = sorted(set(single_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OykD1YRjWuEx"
   },
   "source": [
    "Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZLloj6gTWuEx"
   },
   "outputs": [],
   "source": [
    "# dict <index>:<char>\n",
    "# Your great code here\n",
    "token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
    "# dict <char>:<index>\n",
    "# Your great code here\n",
    "idx_to_token = {j: i for i, j in token_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ONCF6lIPhu9u"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2d09DFdWuEy"
   },
   "source": [
    "*Comment: in this task we have only 38 different tokens, so let's use one-hot encoding.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qusZmvDAWuEy"
   },
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krXxNxOOWuEz"
   },
   "source": [
    "Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.\n",
    "\n",
    "Let's use vanilla RNN, similar to the one created during the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZA5IcBFgWuE0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-32d7a76c8c95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclear_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def to_matrix(text, tokens_to_idx):\n",
    "\n",
    "    # for set_of_phrases in text:\n",
    "    ans = []\n",
    "    for phrase in text:\n",
    "      cur = []\n",
    "      for char in phrase:\n",
    "          cur.append(tokens_to_idx[char.lower()])\n",
    "      rem = MAX_LENGTH - len(cur) ### number of required padding\n",
    "      for i in range(rem):\n",
    "          cur.append(tokens_to_idx[' '])\n",
    "      ans.append(cur)\n",
    "\n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ls6XtTqRfahD"
   },
   "outputs": [],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "class CharRNNLoop(nn.Module):\n",
    "    def __init__(self, num_tokens=len(tokens), emb_size=16, rnn_num_units=64):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n",
    "        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert isinstance(x.data, torch.LongTensor)\n",
    "        h_seq, _ = self.rnn(self.emb(x))\n",
    "        next_logits = self.hid_to_logits(h_seq)\n",
    "        return next_logits\n",
    "\n",
    "    \n",
    "char_rnn = CharRNNLoop()\n",
    "opt = torch.optim.Adam(char_rnn.parameters())\n",
    "history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16vR_a_LWuE0"
   },
   "source": [
    "Plot the loss function (axis X: number of epochs, axis Y: loss function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "BPivzswYWuE0",
    "outputId": "06aced07-be34-4cc8-c6ce-0c1690e711b7"
   },
   "outputs": [],
   "source": [
    "# Your plot code here\n",
    "\n",
    "batch_size = 64\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "for i in range(max_iter):\n",
    "\n",
    "    start_idxs = np.random.randint(0, len(text) - batch_size, size=batch_size)\n",
    "    substrings = [text[start_idx:start_idx+batch_size] for start_idx in start_idxs]\n",
    "    \n",
    "    a = to_matrix(substrings[0], token_to_idx)\n",
    "    \n",
    "    batch_ix = torch.tensor(a, dtype=torch.int64)    \n",
    "    logp_seq = char_rnn(batch_ix)\n",
    "    \n",
    "    # compute loss\n",
    "    predictions_logp = F.log_softmax(logp_seq[:, :-1], dim=-1)\n",
    "    actual_next_tokens = batch_ix[:, 1:]\n",
    "\n",
    "    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))\n",
    "    \n",
    "    # backpropogation\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    if (i+1)%100==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYoh1yNcWuE1"
   },
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
    "                        smaller temperature converges to the single most likely output\n",
    "    '''\n",
    "    \n",
    "    x_sequence = [token_to_idx[token.lower()] for token in seed_phrase]\n",
    "    x_sequence = torch.tensor([[x_sequence]], dtype=torch.int64)\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for i in range(1):\n",
    "        out = char_rnn(x_sequence[:, i])\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length - len(seed_phrase)):\n",
    "        out = char_rnn(x_sequence[:, -1, :])\n",
    "        # Be really careful here with the model output\n",
    "        p_next = F.softmax(out[:, -1, :] / temperature, dim=-1).data.numpy()[0]\n",
    "        \n",
    "        next_ix = np.random.choice(len(tokens), p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix.view((1,1,1))], dim=-1)\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy().squeeze()])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q7qjeWBEeKrz",
    "outputId": "22528133-bb4f-4204-cdae-4a0df0509e46"
   },
   "outputs": [],
   "source": [
    "print(generate_sample(char_rnn, seed_phrase = ' a', temperature = 0.6, max_length=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJtLhmIdWuE2"
   },
   "source": [
    "### More poetic model\n",
    "\n",
    "Let's use LSTM instead of vanilla RNN and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__UJVz1FWuE2"
   },
   "source": [
    "Plot the loss function of the number of epochs. Does the final loss become better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f8e3j6wWuE2"
   },
   "outputs": [],
   "source": [
    "# Your beautiful code here\n",
    "\n",
    "class CharLSTMLoop(nn.Module):\n",
    "    def __init__(self, num_tokens=len(tokens), emb_size=16, rnn_num_units=64):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, rnn_num_units, batch_first=True)\n",
    "        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert isinstance(x.data, torch.LongTensor)\n",
    "        h_seq, _ = self.lstm(self.emb(x))\n",
    "        next_logits = self.hid_to_logits(h_seq)\n",
    "        return next_logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "dc3-BFRapQQ8",
    "outputId": "4dd786fa-16a6-435c-ae8f-2ae7c38f944e"
   },
   "outputs": [],
   "source": [
    "char_lstm = CharLSTMLoop()\n",
    "opt = torch.optim.Adam(char_lstm.parameters())\n",
    "history = []\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(max_iter):\n",
    "\n",
    "    start_idxs = np.random.randint(0, len(text) - batch_size, size=batch_size)\n",
    "    substrings = [text[start_idx:start_idx+batch_size] for start_idx in start_idxs]\n",
    "    a = to_matrix(substrings[0], token_to_idx)\n",
    "    batch_ix = torch.tensor(a, dtype=torch.int64)    \n",
    "    logp_seq = char_lstm(batch_ix)\n",
    "    \n",
    "    # compute loss\n",
    "    predictions_logp = F.log_softmax(logp_seq[:, :-1], dim=-1)\n",
    "    actual_next_tokens = batch_ix[:, 1:]\n",
    "\n",
    "    loss = -torch.mean(torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None]))\n",
    "    # train with backprop\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    history.append(loss.data.numpy())\n",
    "    if (i+1)%100==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2GV7gTfWuE2"
   },
   "source": [
    "Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.\n",
    "\n",
    "Evaluate the results visually, try to interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJI-wZpcWuE2",
    "outputId": "185618a2-f9d7-49b2-a0f8-64c2c099fa00"
   },
   "outputs": [],
   "source": [
    "# Text generation with different temperature values here\n",
    "\n",
    "temperatures = [0.1, 0.2, 0.5, 1.0, 2.0]\n",
    "for temperature in temperatures:\n",
    "    generated_text = generate_sample(char_lstm, seed_phrase = ' a', temperature = temperature)\n",
    "    print ('Text generation with a temperature of', temperature, ':', generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2A1FrZFWuE3"
   },
   "source": [
    "### Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9ZiIUG_WuE3"
   },
   "source": [
    "Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jn1yi9NWuE3"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_dumped = open(\"RNNMODEL5000.pickle\",\"wb+\")\n",
    "pickle.dump(char_rnn, pickle_dumped)\n",
    "\n",
    "pickle_dumped2 = open(\"LSTMMODEL4000.pickle\",\"wb+\")\n",
    "pickle.dump(char_lstm, pickle_dumped2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhC4rwtdogWm",
    "outputId": "43683a13-e801-4d78-c0a0-339be21a333b"
   },
   "outputs": [],
   "source": [
    "pickle_loaded = open(\"RNNMODEL5000.pickle\",\"rb+\")\n",
    "char_rnn_loaded = pickle.load(pickle_loaded)\n",
    "char_rnn_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgKqcK94ojDg"
   },
   "outputs": [],
   "source": [
    "pickle_loaded2 = open(\"LSTMMODEL4000.pickle\",\"rb+\")\n",
    "char_lstm_loaded = pickle.load(pickle_loaded2)\n",
    "char_lstm_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK9Y-vonoluG",
    "outputId": "259f7a20-1154-45a6-ee7d-05c94be2328b"
   },
   "outputs": [],
   "source": [
    "temperature = 0.5\n",
    "print(generate_sample(char_lstm_loaded, seed_phrase = ' Thus', temperature = temperature,max_length= 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6znVU6kWuE3"
   },
   "source": [
    "### References\n",
    "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a> \n",
    "There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.\n",
    "2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>\n",
    "3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch`)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "Copy of Lab2_Poetry_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

Vanishing gradient example:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/master/week1_02_CNN_n_Vanishing_gradient/vanishing_grad_example.ipynb)

LSTM for dealing with sequences:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/girafe-ai/ml-mipt/blob/harbour_masters_ml_s21/day13_LSTM_and_Vanishing_grad/sequence_models_tutorial.ipynb)

How to generate sentences in the autoregressive manner: 
We recommend you to refer to the `Decoder` and `Seq2Seq` classes from this great implementation of the Neural Machine Translation approach. Despite the notebook uses a lot of different techniques, the autoregressive part and `teacher_forcing` idea might be useful
Great notebook on NMT by [bentrevett](https://github.com/bentrevett): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)
